# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cLgeZCRKCtqVUJic9jmcNTOWoVROpLjZ
"""

import numpy as np
import pandas as pd  
from scipy.special import comb
import datetime as dt
import statsmodels.api as sm
import sklearn as sk
from sklearn.pipeline import Pipeline
from sklearn.pipeline import make_pipeline

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures

from sklearn.feature_selection import f_regression
from sklearn.feature_selection import mutual_info_regression
from sklearn.feature_selection import (SelectKBest, SelectPercentile, RFE, RFECV)

from sklearn.linear_model import (LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV)
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import PLSRegression

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate

from sklearn import metrics

from scipy.stats import (randint, loguniform, multivariate_normal)

from collections import Counter

from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import (classification_report, confusion_matrix)
from sklearn.model_selection import GridSearchCV

from tensorflow import keras
from keras import Input
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
plt.style.use('ggplot')
import seaborn as sns
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from sklearn.datasets import fetch_california_housing
California_Housing = fetch_california_housing(as_frame=True)

Cali_Hou=California_Housing.data
Cali_Hou.info()#there are no nan
Cali_Hou.describe()

Cali_Hou['y']=California_Housing.target#the price of the house#
Cali_Hou

X = Cali_Hou.drop(['y'], axis=1)
y = pd.DataFrame(Cali_Hou['y'])
y

scaler        = StandardScaler()
X_stand = scaler.fit_transform(X)
y_stand = scaler.fit_transform(y)

np.transpose(np.array([California_Housing.target.to_numpy()]))

y_stand

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_stand,np.transpose(np.array([California_Housing.target.to_numpy()])), test_size=0.3, shuffle=True,random_state=42)

from tensorflow import keras
from keras import Input
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
plt.style.use('ggplot')
import seaborn as sns

pip install tensorflow_addons

from tensorflow_addons.metrics import RSquare
from tensorflow.keras.optimizers import Adam

FFNN_cali_housing=Sequential(name="FFNN_model_California_Housing") ##benchmark model
FFNN_cali_housing.add(Dense(8, activation='relu', kernel_initializer='he_uniform', input_dim=X_train.shape[1]))
FFNN_cali_housing.add(Dense(18, activation='relu', kernel_initializer='he_uniform'))
FFNN_cali_housing.add(Dense(9, activation='relu', kernel_initializer='he_uniform'))
FFNN_cali_housing.add(Dense(1, activation='linear'))

opt = Adam(learning_rate=.01)
FFNN_cali_housing.compile(
    optimizer=opt,
    loss='mse',
    metrics=["MeanSquaredError",RSquare()]
)

# Step 3: Fit the model
n_epochs     = 200
n_batch_size = 1024

candidate_model = FFNN_cali_housing.fit(
    X_train, 
    y_train, 
    epochs=n_epochs, 
    batch_size=n_batch_size, 
    validation_data=(X_test, y_test)
)

"""Our model gives us 0.7871 R square in validation"""

activasion=["relu",'selu','sigmoid','softmax','elu','linear']
kernel_initializer=['he_uniform','Orthogonal','HeNormal']
#remark:we could also parameterize for learning rate and the loss function but it would take to much time to compile  #
#remark: cross entropy showed terrible rezults mse was more precise we assume this to be due to that we are not optimizing for 0 or 1 type of output of NN #
#remark:we could have alos tried different activation functions for the output layer with an additional for lope but that would also have taken too much time to compile#

modeli=[]

n_epochs     = 200
n_batch_size = 1024
FFNN_cali_housing=Sequential(name="FFNN_model_California_Housing")
for i in activasion:
  for j in kernel_initializer:
    print("activasion: {},kernel_initializer {}".format(i,j))
    FFNN_cali_housing.add(Dense(8, activation=i, kernel_initializer=j, input_dim=X_train.shape[1]))
    FFNN_cali_housing.add(Dense(18, activation=i, kernel_initializer=j))
    FFNN_cali_housing.add(Dense(9, activation=i, kernel_initializer=j))
    FFNN_cali_housing.add(Dense(1, activation=i))
    FFNN_cali_housing.compile(
    optimizer=opt, 
    loss='mse',
    metrics=['accuracy',RSquare()]
    )
    modeli.append(FFNN_cali_housing.fit(
    X_train, 
    y_train, 
    epochs=n_epochs, 
    batch_size=n_batch_size, 
    validation_data=(X_test, y_test)
     ))

rezultati=[]
for i in modeli:
  rezultati.append([max(i.history['val_r_square']),max(i.history['r_square'])])
rezultati

"""The best model in this parameter space gave  R2 of 0.7779150009155273 which is not any improvement from our benchmark model

"""

print(candidate_model.history.keys())##

# Plot history for accuracy
plt.figure(figsize=(24,16))
plt.plot(candidate_model.history['r_square'])
plt.plot(candidate_model.history['val_r_square'])
plt.title('FFNN R2')
plt.ylabel('R2')
plt.xlabel('Epoch')
plt.legend(['Training', 'Testing'], facecolor='white')
plt.show()

# Plot history for accuracy
plt.figure(figsize=(24,16))
plt.plot(candidate_model.history['loss'])
plt.plot(candidate_model.history['val_loss'])
plt.title('FFNN Model Loss Function mse')
plt.ylabel('Loss Function')
plt.xlabel('Epoch')
plt.legend(['Training', 'Testing'], facecolor='white')
plt.show()

"""Our models that are not AI based (LASSO, RIDGE,ELASTIC NET) gave us a R2 of around   0.65.Our AI model gave an R2 of 0.78 in validation which is better."""